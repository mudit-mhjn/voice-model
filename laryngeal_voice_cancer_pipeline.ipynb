{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Voice-Based Benign vs Malignant Laryngeal Disorder Classification\n",
        "**Research-grade pipeline notebook \u2014 generated 2025-10-05 09:42**\n",
        "\n",
        "\n",
        "> **Purpose**: Train and evaluate a robust model that classifies voice recordings into **benign** vs **malignant**, using\n",
        "both **hand-crafted acoustic features** and a **CNN on spectrograms**.  \n",
        "> **Key principles**: subject-wise splits (no leakage), reproducibility, thorough EDA, clinically relevant metrics, and transparent reporting.\n",
        "\n",
        "### What this notebook includes\n",
        "- Data ingestion from two folders: `data/normal-benign/` and `data/malignant/`\n",
        "- Audio **chunking into \u22645s** windows with configurable overlap\n",
        "- Feature extraction matching literature: **F0 stats, jitter, shimmer (APQ3/APQ5), CPP, LTAS bands, ZCR, spectral centroid/flatness, HNR proxy**\n",
        "- Visualizations: distributions, violin plots, correlation heatmaps, **UMAP/PCA**, and **class separability** checks\n",
        "- Two modeling tracks:\n",
        "  1. **Classical ML on features** (LogReg / XGBoost / RF)\n",
        "  2. **CNN on log-mel spectrograms** with **chunk-level** inference aggregated to **file-level**\n",
        "- Evaluation: **AUROC, AUPRC, sensitivity/specificity**, confusion matrix, calibration, **learning curves**, **ablation** and **feature importance (SHAP)** for the classical model\n",
        "- **Reproducibility**: fixed seeds, config cell, environment capture\n",
        "- **Reporting**: model card, limitations, next steps\n",
        "\n",
        "> **Install hint**: First run the `!pip install` cell below on your machine/environment.\n",
        "\n",
        "# \ud83d\udd27 Setup & Requirements\n",
        "\n",
        "This notebook expects the following structure:\n",
        "```\n",
        "project_root/\n",
        "  \u251c\u2500 data/\n",
        "  \u2502   \u251c\u2500 normal-benign/   # .wav/.mp3/etc.\n",
        "  \u2502   \u2514\u2500 malignant/\n",
        "  \u2514\u2500 notebooks/ (optional)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# If running locally/Colab: install dependencies (uncomment if needed)\n",
        "# Note: Some packages may need system libs on local machines.\n",
        "# !pip install numpy pandas scipy librosa soundfile matplotlib scikit-learn umap-learn torch torchvision torchaudio tqdm #                parselmouth-python shap xgboost einops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ========= Configuration ========= #\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = Path('data')\n",
        "BENIGN_DIR = DATA_DIR/'normal-benign'\n",
        "MALIGNANT_DIR = DATA_DIR/'malignant'\n",
        "\n",
        "SAMPLE_RATE = 22050\n",
        "CHUNK_SECONDS = 5.0\n",
        "OVERLAP = 0.5                 # 50% overlap\n",
        "MIN_SNR_DB = None             # set to e.g. 10 to filter low-SNR chunks, or None to disable\n",
        "USE_VAD = True                # basic energy-gate VAD to drop silence\n",
        "SEED = 42\n",
        "N_JOBS = 4                    # parallel workers for feature extraction\n",
        "LOGMEL_N_MELS = 128\n",
        "LOGMEL_HOP = 512\n",
        "LOGMEL_N_FFT = 2048\n",
        "SPEC_AUG = True               # time/freq masking during CNN training\n",
        "SUBJECT_FROM_FILENAME = True  # try to infer subject id from filename prefix before first '_' (customize below)\n",
        "\n",
        "# CNN\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 30\n",
        "LR = 1e-3\n",
        "WEIGHT_DECAY = 1e-4\n",
        "\n",
        "# Cross-validation\n",
        "N_FOLDS = 5                    # subject-wise StratifiedGroupKFold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ========= Imports ========= #\n",
        "import os, re, math, json, random, warnings\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import librosa, soundfile as sf\n",
        "from scipy import signal\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedGroupKFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix, roc_curve, precision_recall_curve, auc, brier_score_loss\n",
        "from sklearn.decomposition import PCA\n",
        "try:\n",
        "    import umap\n",
        "    HAS_UMAP = True\n",
        "except Exception:\n",
        "    HAS_UMAP = False\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from einops import rearrange\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(SEED); random.seed(SEED); torch.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ========= Utility helpers ========= #\n",
        "def set_all_seeds(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def read_audio(path, sr):\n",
        "    y, s = librosa.load(path, sr=sr, mono=True)\n",
        "    return y, s\n",
        "\n",
        "def vad_energy(y, frame_length=2048, hop_length=512, energy_db_thresh=-40):\n",
        "    # Simple energy-based VAD mask\n",
        "    rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length).flatten()\n",
        "    db = 20*np.log10(np.maximum(rms, 1e-10))\n",
        "    active = db > energy_db_thresh\n",
        "    # Expand to samples\n",
        "    mask = np.zeros_like(y, dtype=bool)\n",
        "    for i, a in enumerate(active):\n",
        "        start = i*hop_length\n",
        "        end = start + frame_length\n",
        "        mask[start:end] |= a\n",
        "    return y[mask] if mask.any() else y\n",
        "\n",
        "def chunk_audio(y, sr, chunk_seconds=5.0, overlap=0.5):\n",
        "    L = int(chunk_seconds*sr)\n",
        "    H = int(L*(1-overlap))\n",
        "    if len(y) < L:\n",
        "        # pad with zeros\n",
        "        pad = L - len(y)\n",
        "        y = np.pad(y, (0, pad))\n",
        "    starts = list(range(0, max(1, len(y)-L+1), H if H>0 else L))\n",
        "    chunks = [y[s:s+L] for s in starts]\n",
        "    return chunks\n",
        "\n",
        "def snr_db(y):\n",
        "    if len(y)==0:\n",
        "        return -np.inf\n",
        "    s_power = np.mean(y**2)\n",
        "    n_power = np.var(y - np.mean(y))\n",
        "    if n_power <= 1e-12:\n",
        "        return 100.0\n",
        "    return 10*np.log10(s_power / n_power)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ========= Scan dataset ========= #\n",
        "from glob import glob\n",
        "\n",
        "def infer_subject_id(fname:str):\n",
        "    # Customize: infer subject from filename, e.g., \"ID123_session1.wav\" -> \"ID123\"\n",
        "    base = Path(fname).stem\n",
        "    m = re.match(r'^([^_]+)', base)\n",
        "    return m.group(1) if m else base\n",
        "\n",
        "def list_files():\n",
        "    files = []\n",
        "    for label, d in [('benign', BENIGN_DIR), ('malignant', MALIGNANT_DIR)]:\n",
        "        for ext in ('*.wav','*.mp3','*.flac','*.m4a','*.ogg'):\n",
        "            for f in glob(str(d/ext)):\n",
        "                files.append((f, label))\n",
        "    df = pd.DataFrame(files, columns=['path','label'])\n",
        "    if SUBJECT_FROM_FILENAME:\n",
        "        df['subject'] = df['path'].apply(lambda p: infer_subject_id(p))\n",
        "    else:\n",
        "        df['subject'] = df['path'].apply(lambda p: Path(p).parent.name + '_' + Path(p).stem)\n",
        "    return df\n",
        "\n",
        "df_files = list_files()\n",
        "print(f\"Found {len(df_files)} files (benign={sum(df_files.label=='benign')}, malignant={sum(df_files.label=='malignant')})\")\n",
        "df_files.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ========= Feature extraction (hand-crafted) ========= #\n",
        "# We compute: F0 stats, jitter, shimmer (APQ3/APQ5), CPP, LTAS bands, centroid, flatness, ZCR, HNR proxy.\n",
        "# For jitter/shimmer/CPP we use Praat via parselmouth for robustness."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# If parselmouth fails to import, ensure it is installed\n",
        "try:\n",
        "    import parselmouth\n",
        "    from parselmouth import praat\n",
        "    HAS_PSM = True\n",
        "except Exception as e:\n",
        "    HAS_PSM = False\n",
        "    print(\"[WARN] parselmouth unavailable. Jitter/Shimmer/CPP will be approximated or skipped.\")\n",
        "\n",
        "def praat_jitter_shimmer_cpp(y, sr):\n",
        "    if not HAS_PSM:\n",
        "        return {k: np.nan for k in ['jitter_local','jitter_rap','shimmer_local','shimmer_apq3','shimmer_apq5','cpp']}\n",
        "    snd = parselmouth.Sound(y, sampling_frequency=sr)\n",
        "    # Pitch for period detection\n",
        "    pitch = snd.to_pitch_cc()\n",
        "    point_process = parselmouth.praat.call(snd, \"To PointProcess (periodic, cc)\", 75, 500)\n",
        "    try:\n",
        "        jitter_local = parselmouth.praat.call([snd, point_process], \"Get jitter (local)\", 0, 0, 75, 500, 1.3)\n",
        "        jitter_rap   = parselmouth.praat.call([snd, point_process], \"Get jitter (rap)\",   0, 0, 75, 500, 1.3)\n",
        "        shimmer_local= parselmouth.praat.call([snd, point_process], \"Get shimmer (local)\",0, 0, 75, 500, 1.3, 1.6)\n",
        "        shimmer_apq3 = parselmouth.praat.call([snd, point_process], \"Get shimmer (apq3)\", 0, 0, 75, 500, 1.3, 1.6)\n",
        "        shimmer_apq5 = parselmouth.praat.call([snd, point_process], \"Get shimmer (apq5)\", 0, 0, 75, 500, 1.3, 1.6)\n",
        "        # CPP via cepstrum\n",
        "        cep = parselmouth.praat.call(snd, \"To PowerCepstrum\", 0.01, 0.0001, 0.05)\n",
        "        cpp = parselmouth.praat.call(cep, \"Get peak prominence\", 60, 333)  # in dB\n",
        "    except Exception:\n",
        "        jitter_local=jitter_rap=shimmer_local=shimmer_apq3=shimmer_apq5=cpp=np.nan\n",
        "    return dict(jitter_local=jitter_local, jitter_rap=jitter_rap, shimmer_local=shimmer_local,\n",
        "                shimmer_apq3=shimmer_apq3, shimmer_apq5=shimmer_apq5, cpp=cpp)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def compute_basic_features(y, sr):\n",
        "    # F0\n",
        "    f0 = librosa.yin(y, fmin=75, fmax=500, sr=sr)\n",
        "    f0 = f0[np.isfinite(f0)]\n",
        "    f0_stats = {\n",
        "        'f0_mean': float(np.mean(f0)) if f0.size else np.nan,\n",
        "        'f0_median': float(np.median(f0)) if f0.size else np.nan,\n",
        "        'f0_min': float(np.min(f0)) if f0.size else np.nan,\n",
        "        'f0_max': float(np.max(f0)) if f0.size else np.nan,\n",
        "        'f0_range': float((np.max(f0)-np.min(f0))) if f0.size else np.nan,\n",
        "        'f0_sd': float(np.std(f0)) if f0.size else np.nan\n",
        "    }\n",
        "    # Spectral features\n",
        "    centroid = librosa.feature.spectral_centroid(y=y, sr=sr).mean()\n",
        "    flatness = librosa.feature.spectral_flatness(y=y).mean()\n",
        "    zcr = librosa.feature.zero_crossing_rate(y).mean()\n",
        "    # HNR proxy: harmonic energy over total energy via HPSS\n",
        "    H, P = librosa.effects.hpss(librosa.stft(y))\n",
        "    harm = np.sum(np.abs(H)**2); perc = np.sum(np.abs(P)**2)\n",
        "    hnr_proxy = 10*np.log10((harm + 1e-9)/(perc + 1e-9))\n",
        "    return {\n",
        "        **f0_stats,\n",
        "        'spectral_centroid': float(centroid),\n",
        "        'spectral_flatness': float(flatness),\n",
        "        'zcr': float(zcr),\n",
        "        'hnr_proxy_db': float(hnr_proxy)\n",
        "    }\n",
        "\n",
        "def compute_ltas_bands(y, sr, bands=((0,1000),(1000,2000),(2000,4000),(4000,8000))):\n",
        "    freqs, Pxx = signal.welch(y, sr, nperseg=4096)\n",
        "    total = np.trapz(Pxx, freqs) + 1e-12\n",
        "    out = {}\n",
        "    for (a,b) in bands:\n",
        "        mask = (freqs>=a) & (freqs<b)\n",
        "        band_energy = np.trapz(Pxx[mask], freqs[mask])\n",
        "        out[f'ltas_{a//1000}-{b//1000}k_pct'] = float(100.0*band_energy/total)\n",
        "    return out\n",
        "\n",
        "def extract_handcrafted_features_for_chunk(y, sr):\n",
        "    jitter_shimmer_cpp = praat_jitter_shimmer_cpp(y, sr)\n",
        "    basic = compute_basic_features(y, sr)\n",
        "    ltas = compute_ltas_bands(y, sr)\n",
        "    snr = snr_db(y)\n",
        "    return {**basic, **jitter_shimmer_cpp, **ltas, 'snr_db': snr}\n",
        "\n",
        "def extract_features_from_file(path, label, subject):\n",
        "    y, sr = read_audio(path, SAMPLE_RATE)\n",
        "    if USE_VAD:\n",
        "        y = vad_energy(y)\n",
        "    chunks = chunk_audio(y, sr, CHUNK_SECONDS, OVERLAP)\n",
        "    feats = []\n",
        "    for i, ch in enumerate(chunks):\n",
        "        if MIN_SNR_DB is not None and snr_db(ch) < MIN_SNR_DB:\n",
        "            continue\n",
        "        d = extract_handcrafted_features_for_chunk(ch, sr)\n",
        "        d.update({'path': str(path), 'chunk_idx': i, 'label': label, 'subject': subject})\n",
        "        feats.append(d)\n",
        "    return feats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Run feature extraction (this may take a while)\n",
        "rows = []\n",
        "for _,r in tqdm(df_files.iterrows(), total=len(df_files)):\n",
        "    rows.extend(extract_features_from_file(r['path'], r['label'], r['subject']))\n",
        "df_feats = pd.DataFrame(rows)\n",
        "print(df_feats.shape)\n",
        "df_feats.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploratory Data Analysis\n",
        "- Class balance at **file** and **chunk** level\n",
        "- Distribution plots (e.g., `cpp`, `shimmer_apq3`, `shimmer_apq5`, `f0_max`, `ltas_0-1k_pct`)\n",
        "- Correlations and redundancy\n",
        "- PCA/UMAP overview"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Class balance\n",
        "print('Files ->', df_files.label.value_counts())\n",
        "print('Chunks ->', df_feats.label.value_counts())\n",
        "\n",
        "# Simple distributions with Matplotlib\n",
        "def plot_hist_by_class(feature, bins=40):\n",
        "    plt.figure(figsize=(6,4))\n",
        "    for lab in ['benign','malignant']:\n",
        "        vals = df_feats[df_feats.label==lab][feature].dropna().values\n",
        "        plt.hist(vals, bins=bins, alpha=0.5, label=lab, density=True)\n",
        "    plt.title(feature); plt.legend(); plt.xlabel(feature); plt.ylabel('density'); plt.show()\n",
        "\n",
        "for f in ['cpp','shimmer_apq3','shimmer_apq5','f0_max','ltas_0-1k_pct']:\n",
        "    if f in df_feats.columns:\n",
        "        plot_hist_by_class(f)\n",
        "\n",
        "# Correlation heatmap (matplotlib only)\n",
        "num_cols = [c for c in df_feats.columns if df_feats[c].dtype!=object and c not in ['chunk_idx']]\n",
        "corr = df_feats[num_cols].corr().fillna(0)\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(corr, aspect='auto')\n",
        "plt.colorbar(); plt.title('Feature Correlation'); plt.xticks(range(len(num_cols)), num_cols, rotation=90); plt.yticks(range(len(num_cols)), num_cols); plt.tight_layout(); plt.show()\n",
        "\n",
        "# PCA/UMAP projection\n",
        "X = df_feats[num_cols].fillna(df_feats[num_cols].median())\n",
        "y = (df_feats['label']=='malignant').astype(int).values\n",
        "try:\n",
        "    from sklearn.decomposition import PCA\n",
        "    Xp = PCA(n_components=2).fit_transform(X)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.scatter(Xp[:,0], Xp[:,1], c=y, alpha=0.6)\n",
        "    plt.title('PCA (features)'); plt.xlabel('PC1'); plt.ylabel('PC2'); plt.show()\n",
        "except Exception as e:\n",
        "    print('[WARN] PCA failed:', e)\n",
        "\n",
        "if HAS_UMAP:\n",
        "    Xu = umap.UMAP(random_state=SEED).fit_transform(X)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.scatter(Xu[:,0], Xu[:,1], c=y, alpha=0.6)\n",
        "    plt.title('UMAP (features)'); plt.xlabel('U1'); plt.ylabel('U2'); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ========= Spectrograms & CNN ========= #\n",
        "We will create log-mel spectrograms for each **chunk** and train a compact CNN. During evaluation we\n",
        "aggregate chunk probabilities back to **file** (or **subject**) via mean-probability or majority vote."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def chunk_to_logmel(y, sr, n_mels=LOGMEL_N_MELS, n_fft=LOGMEL_N_FFT, hop_length=LOGMEL_HOP):\n",
        "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, power=2.0)\n",
        "    S_db = librosa.power_to_db(S, ref=np.max)\n",
        "    return S_db.astype(np.float32)\n",
        "\n",
        "class SpectrogramDataset(Dataset):\n",
        "    def __init__(self, df, augment=False):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.augment = augment\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        r = self.df.iloc[idx]\n",
        "        y, sr = read_audio(r['path'], SAMPLE_RATE)\n",
        "        if USE_VAD: y = vad_energy(y)\n",
        "        chunks = chunk_audio(y, sr, CHUNK_SECONDS, OVERLAP)\n",
        "        ch = chunks[int(r['chunk_idx'])]\n",
        "        spec = chunk_to_logmel(ch, sr)\n",
        "        # Normalize per-spec\n",
        "        spec = (spec - spec.mean()) / (spec.std() + 1e-6)\n",
        "        spec = np.expand_dims(spec, 0)  # (1, n_mels, T)\n",
        "        if self.augment:\n",
        "            # simple SpecAugment: random time/freq masking\n",
        "            if np.random.rand()<0.5:\n",
        "                t = spec.shape[-1]; w = int(0.1*t); start = np.random.randint(0, max(1,t-w))\n",
        "                spec[:,:,start:start+w] = 0\n",
        "            if np.random.rand()<0.5:\n",
        "                f = spec.shape[-2]; w = int(0.1*f); start = np.random.randint(0, max(1,f-w))\n",
        "                spec[:,start:start+w,:] = 0\n",
        "        label = 1 if r['label']=='malignant' else 0\n",
        "        return torch.tensor(spec), torch.tensor(label, dtype=torch.long), r['subject'], r['path']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class SmallCNN(nn.Module):\n",
        "    def __init__(self, n_classes=2):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.head = nn.Linear(64* (LOGMEL_N_MELS//8) * ( (int(CHUNK_SECONDS*SAMPLE_RATE/LOGMEL_HOP)+1)//8 ), 128)\n",
        "        self.out = nn.Linear(128, n_classes)\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.flatten(1)\n",
        "        x = F.relu(self.head(x))\n",
        "        x = self.dropout(x)\n",
        "        return self.out(x)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def train_one_epoch(model, loader, opt, device):\n",
        "    model.train(); losses=[]; correct=0; total=0\n",
        "    for xb, yb, _, _ in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        opt.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = F.cross_entropy(logits, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        losses.append(loss.item())\n",
        "        correct += (logits.argmax(1)==yb).sum().item()\n",
        "        total += yb.size(0)\n",
        "    return np.mean(losses), correct/total\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval(); probs=[]; ys=[]; subjects=[]; paths=[] \n",
        "    for xb, yb, subj, p in loader:\n",
        "        xb = xb.to(device)\n",
        "        logits = model(xb)\n",
        "        pr = torch.softmax(logits, dim=1)[:,1].cpu().numpy()\n",
        "        probs.extend(pr); ys.extend(yb.numpy().tolist()); subjects.extend(list(subj)); paths.extend(list(p))\n",
        "    df = pd.DataFrame({'prob': probs, 'y': ys, 'subject': subjects, 'path': paths})\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def aggregate_by(df_chunk, key='path'):\n",
        "    # mean probability per key (path or subject)\n",
        "    agg = df_chunk.groupby(key)['prob'].mean().reset_index()\n",
        "    y = df_chunk.groupby(key)['y'].first().values\n",
        "    return agg['prob'].values, y\n",
        "\n",
        "def evaluate_operating_points(y_true, y_prob, name=''):\n",
        "    fpr, tpr, thr = roc_curve(y_true, y_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    prec, rec, thr2 = precision_recall_curve(y_true, y_prob)\n",
        "    ap = auc(rec, prec)\n",
        "    print(f\"{name} AUROC={roc_auc:.3f} | AUPRC={ap:.3f}\")\n",
        "    # Plot ROC\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.plot(fpr,tpr); plt.plot([0,1],[0,1],'--'); plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title(f'ROC {name}'); plt.show()\n",
        "    # Plot PR\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.plot(rec,prec); plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title(f'PR {name}'); plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ========= Cross-validation (subject-wise) ========= #\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Device:', device)\n",
        "\n",
        "# Prepare fold splits at FILE level (grouped by subject), then expand to chunk rows via merge\n",
        "gkf = StratifiedGroupKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "\n",
        "file_labels = (df_files['label']=='malignant').astype(int).values\n",
        "groups = df_files['subject'].values\n",
        "\n",
        "fold = 0\n",
        "results = []\n",
        "for train_idx, test_idx in gkf.split(df_files, file_labels, groups):\n",
        "    fold += 1\n",
        "    tr_files = df_files.iloc[train_idx]\n",
        "    te_files = df_files.iloc[test_idx]\n",
        "    tr_chunks = df_feats.merge(tr_files[['path']], on='path', how='inner')\n",
        "    te_chunks = df_feats.merge(te_files[['path']], on='path', how='inner')\n",
        "    print(f\"\\n=== Fold {fold}: train chunks={len(tr_chunks)} | test chunks={len(te_chunks)} ===\")\n",
        "\n",
        "    # Classical ML on features\n",
        "    X_cols = [c for c in df_feats.columns if c not in ['path','chunk_idx','label','subject'] and df_feats[c].dtype!=object]\n",
        "    scaler = StandardScaler().fit(tr_chunks[X_cols].fillna(tr_chunks[X_cols].median()))\n",
        "    Xtr = scaler.transform(tr_chunks[X_cols].fillna(tr_chunks[X_cols].median()))\n",
        "    Xte = scaler.transform(te_chunks[X_cols].fillna(tr_chunks[X_cols].median()))\n",
        "    ytr = (tr_chunks['label']=='malignant').astype(int).values\n",
        "    yte = (te_chunks['label']=='malignant').astype(int).values\n",
        "\n",
        "    from xgboost import XGBClassifier\n",
        "    clf = XGBClassifier(n_estimators=300, max_depth=4, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0, random_state=SEED)\n",
        "    clf.fit(Xtr, ytr)\n",
        "\n",
        "    # Chunk-level and file-level eval\n",
        "    pr_tr_chunk = clf.predict_proba(Xtr)[:,1]; pr_te_chunk = clf.predict_proba(Xte)[:,1]\n",
        "    te_pred_df = pd.DataFrame({'prob': pr_te_chunk, 'y': yte, 'path': te_chunks['path'].values, 'subject': te_chunks['subject'].values})\n",
        "\n",
        "    p_file, y_file = aggregate_by(te_pred_df, key='path')\n",
        "    evaluate_operating_points(y_file, p_file, name=f'Classical (Fold {fold}) - file')\n",
        "    p_subj, y_subj = aggregate_by(te_pred_df, key='subject')\n",
        "    evaluate_operating_points(y_subj, p_subj, name=f'Classical (Fold {fold}) - subject')\n",
        "\n",
        "    # CNN track\n",
        "    tr_df_cnn = tr_chunks[['path','chunk_idx','label','subject']].copy()\n",
        "    te_df_cnn = te_chunks[['path','chunk_idx','label','subject']].copy()\n",
        "    ds_tr = SpectrogramDataset(tr_df_cnn, augment=SPEC_AUG)\n",
        "    ds_te = SpectrogramDataset(te_df_cnn, augment=False)\n",
        "    dl_tr = DataLoader(ds_tr, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "    dl_te = DataLoader(ds_te, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "    model = SmallCNN().to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "    best_au = 0; best_state = None\n",
        "    for ep in range(1, EPOCHS+1):\n",
        "        tr_loss, tr_acc = train_one_epoch(model, dl_tr, opt, device)\n",
        "        te_chunk_df = evaluate(model, dl_te, device)\n",
        "        p_file, y_file = aggregate_by(te_chunk_df, key='path')\n",
        "        fpr,tpr,_ = roc_curve(y_file, p_file); au = auc(fpr,tpr)\n",
        "        if au > best_au: best_au, best_state = au, model.state_dict()\n",
        "        if ep % 5 == 0 or ep==1:\n",
        "            print(f\"Epoch {ep}: loss={tr_loss:.3f} acc={tr_acc:.3f} AUROC(file)={au:.3f}\")\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    te_chunk_df = evaluate(model, dl_te, device)\n",
        "    p_file, y_file = aggregate_by(te_chunk_df, key='path')\n",
        "    evaluate_operating_points(y_file, p_file, name=f'CNN (Fold {fold}) - file')\n",
        "    p_subj, y_subj = aggregate_by(te_chunk_df, key='subject')\n",
        "    evaluate_operating_points(y_subj, p_subj, name=f'CNN (Fold {fold}) - subject')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ========= Ablation & Reporting ========= #\n",
        "# Example ablation: restrict classical features to robust markers from literature\n",
        "robust_cols = [c for c in ['f0_max','shimmer_apq3','shimmer_apq5','cpp','ltas_0-1k_pct'] if c in df_feats.columns]\n",
        "if robust_cols:\n",
        "    print('Ablation with robust markers only:', robust_cols)\n",
        "else:\n",
        "    print('[Note] Robust feature columns not present (check extraction).')\n",
        "\n",
        "# Model card (fill with your results)\n",
        "from textwrap import dedent\n",
        "model_card = dedent(f\"\"\"\n",
        "## Model Card \u2014 Laryngeal Voice Classifier\n",
        "\n",
        "**Intended use**: Screening/decision support, not a standalone diagnostic.  \n",
        "**Data**: Benign vs malignant voice recordings; subject-wise CV.  \n",
        "**Preprocessing**: {SAMPLE_RATE} Hz, chunk={CHUNK_SECONDS}s overlap={OVERLAP}. VAD={USE_VAD}.  \n",
        "**Features**: F0 stats, jitter/shimmer, CPP, LTAS, spectral features + log-mel spectrograms for CNN.  \n",
        "**Models**: XGBoost (features), SmallCNN (spectrograms).  \n",
        "**Metrics**: AUROC/AUPRC at file and subject aggregation.  \n",
        "**Calibration**: (add if applied)  \n",
        "**Ablations**: Robust markers only vs full features.  \n",
        "**Bias/Risk**: Potential dataset shift, microphone variability, language content effects.  \n",
        "**Limitations**: Small-N, need external validation and clinical correlation.  \n",
        "**Ethics**: Not a replacement for laryngoscopy/biopsy; inform users about uncertainty.\n",
        "\"\"\")\n",
        "print(model_card)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}